---
phase: 03-hypothesis-synthesis
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - agents/grd-architect.md
autonomous: true

must_haves:
  truths:
    - "OBJECTIVE.md frontmatter can be validated against schema"
    - "Metric weights are validated to sum to 1.0"
    - "Baseline warning is issued if baselines array is empty"
    - "Evaluation methodology is validated for task type"
  artifacts:
    - path: "agents/grd-architect.md"
      provides: "Validation logic embedded in agent"
      contains: "validate"
  key_links:
    - from: "agents/grd-architect.md"
      to: ".planning/OBJECTIVE.md"
      via: "validation before write"
      pattern: "validate.*before"
---

<objective>
Add comprehensive validation logic and baseline warning system to the grd-architect agent.

Purpose: Ensure OBJECTIVE.md documents meet quality standards before generation. Validate metric weights, evaluation methodology compatibility, and enforce baseline soft gate.

Output: Enhanced validation in `agents/grd-architect.md` Step 6
</objective>

<execution_context>
@/Users/evanowen/.claude/get-shit-done/workflows/execute-plan.md
@/Users/evanowen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-hypothesis-synthesis/03-CONTEXT.md
@.planning/phases/03-hypothesis-synthesis/03-RESEARCH.md
@.planning/phases/03-hypothesis-synthesis/03-01-SUMMARY.md
@.planning/phases/03-hypothesis-synthesis/03-02-SUMMARY.md

# Reference for agent update
@agents/grd-architect.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validation logic to grd-architect agent</name>
  <files>agents/grd-architect.md</files>
  <action>
Enhance Step 6 (Validate Completeness) in grd-architect.md with comprehensive validation logic.

**Add validation subsections to Step 6:**

### 6.1 Hypothesis Completeness Validation

Check required elements are present and non-empty:

```python
def validate_hypothesis_completeness(hypothesis_data):
    """
    Validate hypothesis has all required elements.
    Returns (is_valid, errors, warnings).
    """
    errors = []
    warnings = []

    # Required: Hypothesis statement
    if not hypothesis_data.get('hypothesis') or len(hypothesis_data['hypothesis']) < 20:
        errors.append("Hypothesis statement too short or missing (min 20 characters)")

    # Required: Expected outcome
    if not hypothesis_data.get('expected_outcome'):
        errors.append("Expected outcome not specified")

    # Required: At least one metric
    metrics = hypothesis_data.get('metrics', [])
    if not metrics:
        errors.append("No success metrics defined")

    # Required: Evaluation methodology
    if not hypothesis_data.get('evaluation', {}).get('strategy'):
        errors.append("Evaluation methodology not specified")

    # Required: At least one falsification criterion
    falsification = hypothesis_data.get('falsification_criteria', [])
    if not falsification:
        errors.append("No falsification criteria defined")

    # Recommended: Context section
    if not hypothesis_data.get('context') or len(hypothesis_data['context']) < 50:
        warnings.append("Context section is short. Consider adding more background.")

    return (len(errors) == 0, errors, warnings)
```

### 6.2 Metric Weight Validation

Ensure weights sum to 1.0 with tolerance:

```python
def validate_metric_weights(metrics):
    """
    Validate metric weights sum to 1.0.
    Returns (is_valid, message).
    """
    if not metrics:
        return (False, "No metrics defined")

    total_weight = sum(m.get('weight', 0) for m in metrics)

    # Allow small floating point tolerance
    if abs(total_weight - 1.0) > 0.01:
        return (False, f"Metric weights sum to {total_weight:.2f}, should be 1.0")

    # Check individual weights are valid
    for m in metrics:
        weight = m.get('weight', 0)
        if weight < 0 or weight > 1:
            return (False, f"Metric '{m.get('name')}' has invalid weight: {weight}")

    return (True, f"Metric weights valid (sum: {total_weight:.2f})")
```

### 6.3 Evaluation Methodology Validation

Check methodology is appropriate for task:

```python
def validate_evaluation_methodology(evaluation, data_characteristics=None):
    """
    Validate evaluation methodology is appropriate.
    Returns (is_valid, warnings).
    """
    warnings = []
    strategy = evaluation.get('strategy')

    valid_strategies = ['k-fold', 'stratified-k-fold', 'time-series-split', 'holdout']
    if strategy not in valid_strategies:
        return (False, [f"Invalid strategy: {strategy}. Use one of: {valid_strategies}"])

    # Check k-fold has k value
    if 'k-fold' in strategy:
        k = evaluation.get('k_folds')
        if not k or k < 2:
            return (False, ["k-fold strategies require k_folds >= 2"])
        if k > 20:
            warnings.append(f"k={k} is unusually high. Consider k=5 or k=10.")

    # Check holdout has test_size
    if strategy == 'holdout':
        test_size = evaluation.get('test_size')
        if not test_size or test_size < 0.1 or test_size > 0.5:
            warnings.append("Holdout test_size should be between 0.1 and 0.5")

    # Check for temporal data hints
    if data_characteristics:
        has_temporal = data_characteristics.get('has_datetime_columns', False)
        if has_temporal and strategy != 'time-series-split':
            warnings.append("Data has datetime columns. Consider time-series-split to avoid temporal leakage.")

    return (True, warnings)
```

### 6.4 Baseline Soft Gate

Implement baseline warning system:

```python
def check_baseline_definition(baselines):
    """
    Check baseline definition status.
    Returns (has_baseline, warning_message, recommendations).

    This is a SOFT GATE - warns but does not block.
    """
    if not baselines or len(baselines) == 0:
        warning = "WARNING: No baseline defined. Cannot claim improvement without comparison point."
        recommendations = [
            "Option 1: Run your own baseline (e.g., logistic regression, decision tree)",
            "Option 2: Cite literature baseline for this task/dataset",
            "Option 3: Establish random/majority-class baseline for lower bound",
            "You can proceed without baseline but results won't be comparable"
        ]
        return (False, warning, recommendations)

    # Check baseline quality
    own_baselines = [b for b in baselines if b.get('type') == 'own_implementation']
    lit_baselines = [b for b in baselines if b.get('type') == 'literature_citation']

    message = f"Baselines defined: {len(own_baselines)} own, {len(lit_baselines)} literature"
    recommendations = []

    if lit_baselines and not own_baselines:
        recommendations.append("Consider running own baseline to validate literature claims")

    if own_baselines:
        for b in own_baselines:
            if not b.get('expected_performance'):
                recommendations.append(f"Baseline '{b.get('description')}' missing expected performance")

    return (True, message, recommendations)
```

### 6.5 Falsification Criteria Validation

Ensure criteria are meaningful:

```python
def validate_falsification_criteria(criteria, metrics):
    """
    Validate falsification criteria are meaningful.
    Returns (is_valid, warnings).
    """
    warnings = []

    if not criteria:
        return (False, ["No falsification criteria defined - hypothesis is unfalsifiable"])

    metric_names = {m.get('name').lower() for m in metrics}

    for c in criteria:
        # Check metric exists
        criterion_metric = c.get('metric', '').lower()
        if criterion_metric not in metric_names and criterion_metric not in ['any', 'composite']:
            warnings.append(f"Falsification criterion uses metric '{c.get('metric')}' not in success metrics")

        # Check threshold is meaningful
        if c.get('type') == 'quantitative':
            if c.get('threshold') is None:
                warnings.append(f"Quantitative criterion missing threshold")

        # Check explanation exists
        if not c.get('explanation'):
            warnings.append("Falsification criterion missing explanation")

    # Warn if criteria are too lenient
    if len(criteria) == 1 and criteria[0].get('type') == 'qualitative':
        warnings.append("Only qualitative falsification criterion. Consider adding quantitative criteria for objectivity.")

    return (True, warnings)
```

### 6.6 Full Validation Orchestration

Combine all validations:

```python
def validate_objective_before_generation(objective_data, data_characteristics=None):
    """
    Run all validations before generating OBJECTIVE.md.

    Returns:
        {
            'valid': bool,
            'can_proceed': bool,  # True if only warnings, no errors
            'errors': [],
            'warnings': [],
            'baseline_status': {},
            'recommendations': []
        }
    """
    result = {
        'valid': True,
        'can_proceed': True,
        'errors': [],
        'warnings': [],
        'baseline_status': {},
        'recommendations': []
    }

    # 1. Completeness
    complete_valid, errors, warnings = validate_hypothesis_completeness(objective_data)
    if not complete_valid:
        result['valid'] = False
        result['can_proceed'] = False
        result['errors'].extend(errors)
    result['warnings'].extend(warnings)

    # 2. Metric weights
    weights_valid, weights_msg = validate_metric_weights(objective_data.get('metrics', []))
    if not weights_valid:
        result['valid'] = False
        result['can_proceed'] = False
        result['errors'].append(weights_msg)

    # 3. Evaluation methodology
    eval_valid, eval_warnings = validate_evaluation_methodology(
        objective_data.get('evaluation', {}),
        data_characteristics
    )
    if not eval_valid:
        result['valid'] = False
        result['can_proceed'] = False
        result['errors'].extend(eval_warnings)
    else:
        result['warnings'].extend(eval_warnings)

    # 4. Baseline (soft gate - warns but allows proceeding)
    baseline_defined, baseline_msg, baseline_recs = check_baseline_definition(
        objective_data.get('baselines', [])
    )
    result['baseline_status'] = {
        'defined': baseline_defined,
        'message': baseline_msg
    }
    if not baseline_defined:
        result['warnings'].append(baseline_msg)
        result['recommendations'].extend(baseline_recs)
    # Note: baseline missing does NOT set can_proceed=False

    # 5. Falsification criteria
    fals_valid, fals_warnings = validate_falsification_criteria(
        objective_data.get('falsification_criteria', []),
        objective_data.get('metrics', [])
    )
    if not fals_valid:
        result['valid'] = False
        result['can_proceed'] = False
        result['errors'].extend(fals_warnings)
    else:
        result['warnings'].extend(fals_warnings)

    return result
```

**Present validation results to user:**

After validation, present results:

```
## Validation Results

### Errors (must fix before proceeding)
{errors_list or "None"}

### Warnings (review recommended)
{warnings_list or "None"}

### Baseline Status
{baseline_status_message}
{baseline_recommendations if applicable}

---

{if can_proceed}
Proceed with OBJECTIVE.md generation? (yes/no)

{else}
Please address the errors above before proceeding.
{ask for corrections}
```

**Important:** Baseline missing is a WARNING only. User can proceed. All other validation failures are ERRORS that must be fixed.
  </action>
  <verify>
File updated: `grep "validate" agents/grd-architect.md`
Contains weight validation: `grep "weight.*sum" agents/grd-architect.md`
Contains baseline soft gate: `grep -i "soft.gate\|WARNING.*baseline" agents/grd-architect.md`
Contains evaluation validation: `grep "time-series-split" agents/grd-architect.md`
Contains falsification validation: `grep "unfalsifiable" agents/grd-architect.md`
  </verify>
  <done>
grd-architect agent enhanced with validation logic:
- Hypothesis completeness validation (required elements)
- Metric weight validation (sum to 1.0)
- Evaluation methodology validation (appropriate for task)
- Baseline soft gate (warns but allows proceeding)
- Falsification criteria validation (meaningful criteria)
- Validation orchestration presenting errors vs warnings
  </done>
</task>

<task type="auto">
  <name>Task 2: Add data constraints integration</name>
  <files>agents/grd-architect.md</files>
  <action>
Enhance Step 1 (Load Context) to extract data constraints from DATA_REPORT.md and integrate into validation.

**Add to Step 1:**

### 1.3 Extract Data Characteristics for Validation

If DATA_REPORT.md exists, extract characteristics that affect validation:

```python
def extract_data_characteristics(data_report_path):
    """
    Extract characteristics from DATA_REPORT.md for hypothesis validation.
    """
    characteristics = {
        'has_datetime_columns': False,
        'has_class_imbalance': False,
        'imbalance_severity': None,
        'leakage_warnings': [],
        'missing_data_columns': [],
        'high_cardinality_columns': [],
        'sample_size': None
    }

    # Parse DATA_REPORT.md
    # Look for datetime column indicators
    # Look for class balance section
    # Look for leakage warnings (HIGH confidence)
    # Look for missing data patterns
    # Extract row count

    return characteristics
```

**Add to Step 6:**

Pass data characteristics to validation:

```python
# In validate_objective_before_generation
data_chars = extract_data_characteristics('.planning/DATA_REPORT.md')

# Use in evaluation methodology validation
eval_valid, eval_warnings = validate_evaluation_methodology(
    objective_data.get('evaluation', {}),
    data_chars  # Pass data characteristics
)

# Additional data-informed warnings
if data_chars['has_class_imbalance'] and data_chars['imbalance_severity'] == 'HIGH':
    if 'accuracy' in [m.get('name', '').lower() for m in objective_data.get('metrics', [])]:
        warnings.append("High class imbalance detected. Consider using F1, precision/recall, or AUC instead of accuracy.")

# Warn about leakage features in hypothesis
if data_chars['leakage_warnings']:
    for warning in data_chars['leakage_warnings']:
        if warning['confidence'] == 'HIGH':
            warnings.append(f"DATA_REPORT.md flagged potential leakage: {warning['feature']}. Exclude from hypothesis if using this feature.")
```

**Add constraint section to OBJECTIVE.md:**

If data constraints exist, automatically populate Constraints section:

```python
def generate_constraints_section(data_chars):
    """
    Generate Constraints section from data characteristics.
    """
    constraints = []

    if data_chars['has_class_imbalance']:
        constraints.append(f"Class imbalance ({data_chars['imbalance_severity']}): Consider stratified sampling or class weights")

    if data_chars['leakage_warnings']:
        for w in data_chars['leakage_warnings']:
            if w['confidence'] == 'HIGH':
                constraints.append(f"Exclude feature '{w['feature']}' due to potential leakage (confidence: HIGH)")

    if data_chars['missing_data_columns']:
        cols = ', '.join(data_chars['missing_data_columns'][:5])
        constraints.append(f"Handle missing data in: {cols}")

    return constraints
```
  </action>
  <verify>
Contains data characteristics extraction: `grep "extract_data_characteristics" agents/grd-architect.md`
Contains class imbalance warning: `grep "class.imbalance\|imbalance" agents/grd-architect.md`
Contains leakage integration: `grep "leakage_warnings" agents/grd-architect.md`
Contains constraints generation: `grep "generate_constraints_section" agents/grd-architect.md`
  </verify>
  <done>
grd-architect agent enhanced with data constraints integration:
- DATA_REPORT.md characteristics extraction
- Data-informed validation warnings
- Automatic constraints section population
- Leakage feature warnings
- Class imbalance metric recommendations
  </done>
</task>

</tasks>

<verification>
- [ ] Validation logic added to Step 6 of grd-architect agent
- [ ] Metric weights validated to sum to 1.0
- [ ] Baseline soft gate issues warning (not error) when missing
- [ ] Evaluation methodology validated for task type
- [ ] Falsification criteria validated for meaningfulness
- [ ] Data characteristics extracted from DATA_REPORT.md
- [ ] Constraints auto-populated from data characteristics
</verification>

<success_criteria>
OBJECTIVE.md generation is guarded by comprehensive validation. Users receive clear feedback on errors (must fix) vs warnings (review recommended). Baseline missing is a soft gate that warns but allows proceeding.
</success_criteria>

<output>
After completion, create `.planning/phases/03-hypothesis-synthesis/03-03-SUMMARY.md`
</output>
