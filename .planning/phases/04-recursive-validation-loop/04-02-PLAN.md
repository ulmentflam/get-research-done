---
phase: 04-recursive-validation-loop
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/grd-critic.md
  - get-research-done/templates/critic-log.md
autonomous: true

must_haves:
  truths:
    - "Critic agent can evaluate experiment against OBJECTIVE.md criteria"
    - "Critic produces structured verdict with confidence level"
    - "Critic routes to correct next step (PROCEED/REVISE_METHOD/REVISE_DATA/ESCALATE)"
  artifacts:
    - path: "agents/grd-critic.md"
      provides: "Critic agent with LLM-based routing logic"
      min_lines: 250
    - path: "get-research-done/templates/critic-log.md"
      provides: "Template for CRITIC_LOG.md files"
      min_lines: 40
  key_links:
    - from: "agents/grd-critic.md"
      to: ".planning/OBJECTIVE.md"
      via: "file read"
      pattern: "OBJECTIVE.md"
    - from: "agents/grd-critic.md"
      to: "experiments/run_NNN/CRITIC_LOG.md"
      via: "file write"
      pattern: "CRITIC_LOG"
---

<objective>

Create the grd-critic agent that audits experiments with skeptical evaluation and LLM-based routing decisions.

Purpose: Critic is the scientific skeptic of the recursive validation loop. It evaluates experiments against OBJECTIVE.md success criteria, detects suspicious patterns (overfitting, leakage), and determines routing: PROCEED to Evaluator, REVISE_METHOD back to Researcher, REVISE_DATA back to Explorer, or ESCALATE to human.

Output:
- grd-critic agent with LLM-based evaluation and routing
- CRITIC_LOG.md template for structured critique output

</objective>

<execution_context>

@/Users/evanowen/.claude/get-research-done/workflows/execute-plan.md
@/Users/evanowen/.claude/get-research-done/templates/summary.md

</execution_context>

<context>

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-recursive-validation-loop/04-CONTEXT.md
@.planning/phases/04-recursive-validation-loop/04-RESEARCH.md

Reference existing patterns:
@agents/grd-architect.md
@agents/grd-explorer.md
@get-research-done/templates/objective.md

</context>

<tasks>

<task type="auto">
  <name>Task 1: Create grd-critic agent</name>
  <files>agents/grd-critic.md</files>
  <action>
Create the critic agent following established patterns.

Structure:
```markdown
---
name: grd-critic
description: Audits experiments with skeptical evaluation and LLM-based routing decisions
tools: Read, Write, Bash, Glob, Grep
color: red
---
```

Include:
1. Role section explaining Critic as scientific skeptic:
   - Evaluates against OBJECTIVE.md success criteria first
   - Applies broader scientific skepticism
   - Flags suspicious success (unusually high metrics)
   - LLM reasoning for routing (not rule-based)
   - Provides actionable feedback for REVISE paths

2. Execution flow with 7 steps:

**Step 1: Load Context**
- Read OBJECTIVE.md: success metrics, thresholds, evaluation methodology, falsification criteria
- Read experiment code from run directory
- Read metrics from experiment output
- Read previous CRITIC_LOGs to track history and avoid cycles
- Parse iteration count from state

**Step 2: Evaluate Against Success Criteria**
- Compare each metric against OBJECTIVE.md thresholds
- Calculate weighted composite score
- Note which criteria pass/fail
- Check if baseline comparison available

**Step 3: Apply Scientific Skepticism**
Key checks (LLM reasoning, not rules):
- **Suspicious success**: Metrics unusually high (>95% accuracy on complex task)? Investigate overfitting/leakage signs
- **Train-test gap**: Large difference between train and validation performance?
- **Trend analysis**: Are metrics improving across iterations or stagnant?
- **Code quality**: Does implementation match methodology in OBJECTIVE.md?
- **Data integrity**: Any signs of leakage patterns from DATA_REPORT.md?
- **Reproducibility**: Is random seed set? Is code deterministic?

**Step 4: Determine Verdict**
Use LLM reasoning to decide routing:

- **PROCEED** (confidence required):
  - All success criteria met or exceeded
  - No suspicious patterns detected
  - Implementation aligns with methodology
  - Confidence: HIGH (no concerns), MEDIUM (minor notes), LOW (gate to human)

- **REVISE_METHOD**:
  - Metrics below threshold due to implementation issues
  - Hyperparameters need tuning
  - Code bugs or methodology errors
  - Provide specific actionable recommendations

- **REVISE_DATA**:
  - Anomalous results suggesting data issues
  - Leakage detected in execution
  - Data drift or quality problems surfaced
  - Route back to Explorer with specific concerns

- **ESCALATE**:
  - Cannot determine root cause
  - Ambiguous failure mode
  - Multiple conflicting signals
  - Surface to human for strategic decision

**Step 5: Generate Structured Critique**
Output format (Pydantic-like structure):
```
Strengths: [list of what experiment does well]
Weaknesses: [list of issues or concerns]
Verdict: PROCEED | REVISE_METHOD | REVISE_DATA | ESCALATE
Confidence: HIGH | MEDIUM | LOW
Recommendations: [list of specific actionable suggestions]
Reasoning: [explanation of routing decision]
Metrics Summary: {metric: {value, threshold, pass/fail}}
Trend: [improving | stagnant | degrading] (if multiple iterations)
```

**Step 6: Write CRITIC_LOG.md**
- Write structured critique to experiments/run_NNN/CRITIC_LOG.md
- Include all fields from structured output
- Reference OBJECTIVE.md criteria
- Include iteration number and timestamp

**Step 7: Return Verdict**
- Return verdict and confidence to calling agent (Researcher)
- If PROCEED with HIGH/MEDIUM confidence: ready for Evaluator
- If PROCEED with LOW confidence: route to human for confirmation
- If REVISE_*: include specific recommendations
- If ESCALATE: prepare evidence package for human

3. Quality gates:
- Never proceed with LOW confidence without human gate
- Suspicious success (>95% on complex task) always triggers investigation
- Repeated same verdict (3x) triggers escalation

4. Edge cases:
- No previous CRITIC_LOGs (first iteration): proceed normally
- Metrics missing from output: REVISE_METHOD with "collect metrics" recommendation
- OBJECTIVE.md baseline not defined: warn but don't block on that alone

5. Key behaviors from CONTEXT.md:
- LLM reasoning for routing (not rules-based)
- Full history access (sees all previous CRITIC_LOGs)
- Both levels of feedback: general + specific
- Flag suspicious success
- Confidence level in every verdict
- Track trends across runs
  </action>
  <verify>
```bash
# File exists with correct structure
test -f agents/grd-critic.md && echo "Agent file exists"

# Has required sections
grep -q "name: grd-critic" agents/grd-critic.md && echo "Has frontmatter"
grep -q "Step 1:" agents/grd-critic.md && echo "Has step 1"
grep -q "Step 7:" agents/grd-critic.md && echo "Has step 7"
grep -q "PROCEED" agents/grd-critic.md && echo "Has PROCEED verdict"
grep -q "REVISE_METHOD" agents/grd-critic.md && echo "Has REVISE_METHOD verdict"
grep -q "REVISE_DATA" agents/grd-critic.md && echo "Has REVISE_DATA verdict"
grep -q "ESCALATE" agents/grd-critic.md && echo "Has ESCALATE verdict"
grep -q "confidence" agents/grd-critic.md && echo "References confidence levels"
grep -q "CRITIC_LOG" agents/grd-critic.md && echo "References CRITIC_LOG"
```
  </verify>
  <done>Agent file exists with 7-step workflow, LLM-based routing logic, all four verdicts (PROCEED/REVISE_METHOD/REVISE_DATA/ESCALATE), confidence levels, and structured critique output</done>
</task>

<task type="auto">
  <name>Task 2: Create CRITIC_LOG.md template</name>
  <files>get-research-done/templates/critic-log.md</files>
  <action>
Create template for structured critique output written to each run directory.

Structure:
```markdown
# Critic Evaluation: {{run_name}}

**Timestamp:** {{timestamp}}
**Iteration:** {{iteration_number}}
**Objective:** {{brief_hypothesis}}

---

## Verdict

**Decision:** {{PROCEED | REVISE_METHOD | REVISE_DATA | ESCALATE}}
**Confidence:** {{HIGH | MEDIUM | LOW}}

## Reasoning

{{explanation_of_routing_decision}}

## Metrics Summary

| Metric | Value | Threshold | Comparison | Result |
|--------|-------|-----------|------------|--------|
| {{metric_name}} | {{value}} | {{threshold}} | {{>|<|=}} | {{PASS|FAIL}} |

**Composite Score:** {{weighted_average}} (threshold: {{composite_threshold}})

## Strengths

{{list_of_what_experiment_does_well}}

## Weaknesses

{{list_of_issues_or_concerns}}

## Recommendations

{{list_of_specific_actionable_suggestions}}

## Investigation Notes

{{notes_from_scientific_skepticism_checks}}

- Suspicious success check: {{result}}
- Train-test gap: {{result}}
- Reproducibility: {{result}}
- Data integrity: {{result}}

## Trend Analysis

**Iteration Trend:** {{improving | stagnant | degrading | first_run}}

{{comparison_with_previous_iterations_if_available}}

## Next Steps

{{based_on_verdict}}

- PROCEED: Ready for quantitative evaluation by Evaluator
- REVISE_METHOD: Address recommendations, re-run experiment
- REVISE_DATA: Return to /grd:explore with concerns: {{specific_concerns}}
- ESCALATE: Human decision required - see evidence package

---

*Critique by grd-critic*
*Referenced: .planning/OBJECTIVE.md*
```

Include placeholders for:
- Run identification (name, timestamp, iteration)
- Verdict and confidence
- Reasoning explanation
- Metrics table with pass/fail
- Strengths and weaknesses lists
- Actionable recommendations
- Investigation notes (skepticism checks)
- Trend analysis
- Next steps based on verdict
  </action>
  <verify>
```bash
# File exists
test -f get-research-done/templates/critic-log.md && echo "Template exists"

# Has required placeholders
grep -q "{{run_name}}" get-research-done/templates/critic-log.md && echo "Has run_name"
grep -q "Verdict" get-research-done/templates/critic-log.md && echo "Has Verdict section"
grep -q "Confidence" get-research-done/templates/critic-log.md && echo "Has Confidence"
grep -q "Strengths" get-research-done/templates/critic-log.md && echo "Has Strengths"
grep -q "Weaknesses" get-research-done/templates/critic-log.md && echo "Has Weaknesses"
grep -q "Recommendations" get-research-done/templates/critic-log.md && echo "Has Recommendations"
grep -q "Trend" get-research-done/templates/critic-log.md && echo "Has Trend Analysis"
```
  </verify>
  <done>Template exists with all required sections: verdict, confidence, reasoning, metrics table, strengths/weaknesses, recommendations, investigation notes, trend analysis, and next steps</done>
</task>

</tasks>

<verification>

After all tasks complete:
1. grd-critic agent has 7-step workflow
2. Agent uses LLM reasoning (not rules) for routing
3. All four verdicts documented with criteria
4. Confidence levels (HIGH/MEDIUM/LOW) required
5. CRITIC_LOG.md template has structured format
6. Agent references OBJECTIVE.md for criteria
7. Agent tracks iteration history to avoid cycles

</verification>

<success_criteria>

- [ ] Agent file at agents/grd-critic.md
- [ ] Template file at get-research-done/templates/critic-log.md
- [ ] Agent has all four verdicts: PROCEED, REVISE_METHOD, REVISE_DATA, ESCALATE
- [ ] Each verdict includes confidence level
- [ ] LLM reasoning documented (not rule-based routing)
- [ ] Suspicious success detection included
- [ ] History tracking to avoid cycles
- [ ] Structured output format matches template

</success_criteria>

<output>

After completion, create `.planning/phases/04-recursive-validation-loop/04-02-SUMMARY.md`

</output>
