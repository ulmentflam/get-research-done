---
phase: 04-recursive-validation-loop
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/grd-evaluator.md
  - get-research-done/templates/scorecard.json
autonomous: true

must_haves:
  truths:
    - "Evaluator agent produces quantitative SCORECARD.json"
    - "SCORECARD includes metrics against OBJECTIVE.md success criteria"
    - "Evaluator logs to MLflow if available"
  artifacts:
    - path: "agents/grd-evaluator.md"
      provides: "Evaluator agent with quantitative benchmarking"
      min_lines: 150
    - path: "get-research-done/templates/scorecard.json"
      provides: "JSON schema for SCORECARD output"
      min_lines: 30
  key_links:
    - from: "agents/grd-evaluator.md"
      to: ".planning/OBJECTIVE.md"
      via: "file read"
      pattern: "OBJECTIVE.md"
    - from: "agents/grd-evaluator.md"
      to: "experiments/run_NNN/metrics/SCORECARD.json"
      via: "file write"
      pattern: "SCORECARD"
---

<objective>

Create the grd-evaluator agent that performs quantitative benchmarking and produces SCORECARD.json.

Purpose: Evaluator is the final validation step when Critic says PROCEED. It runs standardized benchmarks, computes metrics against OBJECTIVE.md success criteria, calculates composite scores, and generates the structured SCORECARD.json that feeds into Phase 5's human evaluation gate.

Output:
- grd-evaluator agent with benchmarking workflow
- SCORECARD.json template/schema

</objective>

<execution_context>

@/Users/evanowen/.claude/get-research-done/workflows/execute-plan.md
@/Users/evanowen/.claude/get-research-done/templates/summary.md

</execution_context>

<context>

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-recursive-validation-loop/04-CONTEXT.md
@.planning/phases/04-recursive-validation-loop/04-RESEARCH.md

Reference existing patterns:
@agents/grd-architect.md
@get-research-done/templates/objective.md

</context>

<tasks>

<task type="auto">
  <name>Task 1: Create grd-evaluator agent</name>
  <files>agents/grd-evaluator.md</files>
  <action>
Create the evaluator agent following established patterns.

Structure:
```markdown
---
name: grd-evaluator
description: Performs quantitative benchmarking and generates SCORECARD.json for validated experiments
tools: Read, Write, Bash, Glob, Grep
color: yellow
---
```

Include:
1. Role section explaining Evaluator performs final quantitative validation:
   - Only runs after Critic PROCEED verdict
   - Computes metrics against OBJECTIVE.md success criteria
   - Generates standardized SCORECARD.json
   - Optionally logs to MLflow if configured
   - Prepares evidence package for Phase 5 human gate

2. Execution flow with 6 steps:

**Step 1: Load Context**
- Read OBJECTIVE.md: success metrics with thresholds, weights, evaluation methodology
- Read experiment code and outputs from run directory
- Read CRITIC_LOG.md to confirm PROCEED verdict
- Parse run metadata (iteration, timestamp, description)

**Step 2: Verify Critic Approval**
- Check CRITIC_LOG.md exists and contains PROCEED verdict
- If no CRITIC_LOG or non-PROCEED verdict: abort with error
- Note confidence level from Critic

**Step 3: Run Evaluation**
Based on OBJECTIVE.md evaluation methodology:
- **k-fold CV**: Execute k-fold cross-validation, aggregate metrics
- **stratified-k-fold**: Same but preserve class distribution
- **time-series-split**: Temporal train/test split
- **holdout**: Single train/test split evaluation

For each fold/split:
- Run experiment code
- Collect metrics (accuracy, F1, AUC, RMSE, etc.)
- Record per-fold results

**Step 4: Compute Metrics**
- Calculate mean and std for each metric across folds
- Compare against OBJECTIVE.md thresholds
- Compute weighted composite score using metric weights
- Determine overall pass/fail
- Compare against baseline if defined in OBJECTIVE.md

**Step 5: Generate SCORECARD.json**
Write to experiments/run_NNN/metrics/SCORECARD.json:
```json
{
  "run_id": "run_001_baseline",
  "timestamp": "2026-01-28T12:00:00Z",
  "objective_ref": ".planning/OBJECTIVE.md",
  "hypothesis": "Brief hypothesis statement",
  "iteration": 1,
  "data_version": "sha256:abc123...",
  "evaluation": {
    "strategy": "stratified-k-fold",
    "k": 5,
    "random_state": 42
  },
  "metrics": {
    "accuracy": {"mean": 0.87, "std": 0.02, "threshold": 0.85, "comparison": ">=", "weight": 0.3, "result": "PASS"},
    "f1_score": {"mean": 0.82, "std": 0.03, "threshold": 0.80, "comparison": ">=", "weight": 0.5, "result": "PASS"},
    "precision": {"mean": 0.84, "std": 0.02, "threshold": 0.75, "comparison": ">=", "weight": 0.2, "result": "PASS"}
  },
  "composite_score": 0.84,
  "composite_threshold": 0.80,
  "overall_result": "PASS",
  "baseline_comparison": {
    "baseline_name": "logistic_regression",
    "baseline_score": 0.72,
    "improvement": 0.12,
    "significant": true
  },
  "confidence_interval": {
    "composite_lower": 0.81,
    "composite_upper": 0.87,
    "confidence_level": 0.95
  },
  "critic_verdict": "PROCEED",
  "critic_confidence": "HIGH",
  "ready_for_human_review": true
}
```

**Step 6: Optional MLflow Logging**
If MLflow is configured (check for mlflow in environment):
- Log parameters (hyperparameters, evaluation settings)
- Log metrics (all metrics and composite score)
- Log artifacts (SCORECARD.json, model outputs)
- Tag run with hypothesis and verdict

If MLflow not available, skip silently (no error).

3. Quality gates:
- All metrics from OBJECTIVE.md must be computed
- Composite score must be calculated with correct weights
- Data version hash must be recorded for provenance

4. Return format:
- Return structured result with overall_result (PASS/FAIL)
- Include SCORECARD.json path
- Signal ready for Phase 5 human review
  </action>
  <verify>
```bash
# File exists with correct structure
test -f agents/grd-evaluator.md && echo "Agent file exists"

# Has required sections
grep -q "name: grd-evaluator" agents/grd-evaluator.md && echo "Has frontmatter"
grep -q "Step 1:" agents/grd-evaluator.md && echo "Has step 1"
grep -q "Step 6:" agents/grd-evaluator.md && echo "Has step 6"
grep -q "SCORECARD" agents/grd-evaluator.md && echo "References SCORECARD"
grep -q "OBJECTIVE.md" agents/grd-evaluator.md && echo "References OBJECTIVE.md"
grep -q "MLflow" agents/grd-evaluator.md && echo "References MLflow"
grep -q "composite" agents/grd-evaluator.md && echo "References composite score"
```
  </verify>
  <done>Agent file exists with 6-step workflow covering context loading, Critic verification, evaluation execution, metrics computation, SCORECARD generation, and MLflow logging</done>
</task>

<task type="auto">
  <name>Task 2: Create SCORECARD.json template</name>
  <files>get-research-done/templates/scorecard.json</files>
  <action>
Create a JSON schema/template for SCORECARD output.

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "grd-scorecard-v1",
  "title": "GRD Experiment Scorecard",
  "description": "Quantitative evaluation results for a validated experiment run",

  "run_id": "{{run_NNN_description}}",
  "timestamp": "{{ISO8601_timestamp}}",
  "objective_ref": ".planning/OBJECTIVE.md",
  "hypothesis": "{{brief_hypothesis_statement}}",
  "iteration": "{{iteration_number}}",
  "data_version": "{{sha256_hash_of_data}}",

  "evaluation": {
    "strategy": "{{k-fold|stratified-k-fold|time-series-split|holdout}}",
    "k": "{{number_of_folds_or_null}}",
    "test_size": "{{proportion_or_null}}",
    "random_state": 42,
    "folds_completed": "{{number}}"
  },

  "metrics": {
    "{{metric_name}}": {
      "mean": "{{float}}",
      "std": "{{float}}",
      "per_fold": ["{{fold_1_value}}", "{{fold_2_value}}"],
      "threshold": "{{from_objective}}",
      "comparison": "{{>=|<=|==}}",
      "weight": "{{0.0-1.0}}",
      "result": "{{PASS|FAIL}}"
    }
  },

  "composite_score": "{{weighted_average}}",
  "composite_threshold": "{{from_objective_or_default_0.5}}",
  "overall_result": "{{PASS|FAIL}}",

  "baseline_comparison": {
    "baseline_name": "{{name_or_null}}",
    "baseline_source": "{{own_implementation|literature_citation}}",
    "baseline_score": "{{float_or_null}}",
    "improvement": "{{float_difference}}",
    "improvement_pct": "{{percentage}}",
    "significant": "{{true|false|not_tested}}"
  },

  "confidence_interval": {
    "composite_lower": "{{float}}",
    "composite_upper": "{{float}}",
    "confidence_level": 0.95,
    "method": "{{bootstrap|t_distribution}}"
  },

  "provenance": {
    "code_snapshot": "experiments/{{run_id}}/code/",
    "config_file": "experiments/{{run_id}}/config.yaml",
    "logs": "experiments/{{run_id}}/logs/",
    "outputs": "experiments/{{run_id}}/outputs/"
  },

  "critic_summary": {
    "verdict": "PROCEED",
    "confidence": "{{HIGH|MEDIUM|LOW}}",
    "log_path": "experiments/{{run_id}}/CRITIC_LOG.md"
  },

  "ready_for_human_review": true,
  "next_phase": "Phase 5: Human Evaluation Gate"
}
```

Include:
- Run identification (run_id, timestamp, iteration)
- Objective reference and hypothesis summary
- Data version for reproducibility
- Evaluation methodology details
- Metrics with mean, std, per-fold, threshold, result
- Composite score calculation
- Baseline comparison (if available)
- Confidence intervals
- Provenance links (code, config, logs, outputs)
- Critic summary
- Phase 5 readiness flag
  </action>
  <verify>
```bash
# File exists
test -f get-research-done/templates/scorecard.json && echo "Template exists"

# Has required fields (checking as text since it's a template with placeholders)
grep -q "run_id" get-research-done/templates/scorecard.json && echo "Has run_id"
grep -q "metrics" get-research-done/templates/scorecard.json && echo "Has metrics"
grep -q "composite_score" get-research-done/templates/scorecard.json && echo "Has composite_score"
grep -q "overall_result" get-research-done/templates/scorecard.json && echo "Has overall_result"
grep -q "baseline_comparison" get-research-done/templates/scorecard.json && echo "Has baseline"
grep -q "confidence_interval" get-research-done/templates/scorecard.json && echo "Has confidence"
grep -q "provenance" get-research-done/templates/scorecard.json && echo "Has provenance"
```
  </verify>
  <done>Template exists with complete schema including run identification, evaluation details, metrics with aggregation, composite score, baseline comparison, confidence intervals, and provenance links</done>
</task>

</tasks>

<verification>

After all tasks complete:
1. grd-evaluator agent has 6-step workflow
2. Agent verifies Critic PROCEED before running
3. Agent computes metrics per OBJECTIVE.md methodology
4. SCORECARD.json has all required fields
5. Composite score uses correct weights
6. MLflow integration is optional (graceful skip)
7. Phase 5 readiness flagged

</verification>

<success_criteria>

- [ ] Agent file at agents/grd-evaluator.md
- [ ] Template file at get-research-done/templates/scorecard.json
- [ ] Agent verifies Critic PROCEED verdict
- [ ] Agent computes metrics per OBJECTIVE.md evaluation methodology
- [ ] Agent calculates weighted composite score
- [ ] SCORECARD.json includes all metrics with pass/fail
- [ ] Baseline comparison included if available
- [ ] Confidence intervals calculated
- [ ] MLflow logging optional (no error if unavailable)

</success_criteria>

<output>

After completion, create `.planning/phases/04-recursive-validation-loop/04-03-SUMMARY.md`

</output>
