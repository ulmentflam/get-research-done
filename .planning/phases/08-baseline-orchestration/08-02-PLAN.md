---
phase: 08-baseline-orchestration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/grd-evaluator.md
autonomous: true

must_haves:
  truths:
    - "Evaluator verifies baseline still exists before generating SCORECARD"
    - "SCORECARD includes multi-baseline comparison table"
    - "Missing baseline at evaluation time logged as warning in SCORECARD"
    - "Baseline comparison shows improvement for each available baseline"
  artifacts:
    - path: "agents/grd-evaluator.md"
      provides: "Baseline safety check and multi-baseline comparison"
      contains: "Step 1.5: Validate Baseline Availability"
  key_links:
    - from: "agents/grd-evaluator.md"
      to: "experiments/run_*/metrics.json"
      via: "Baseline metrics loading"
      pattern: "baseline.*metrics"
---

<objective>
Add baseline safety check to grd-evaluator agent and enhance SCORECARD generation with multi-baseline comparison table.

Purpose: Ensure baseline comparison integrity at evaluation time and provide comprehensive comparison across all available baselines for hypothesis validation.

Output: Updated grd-evaluator.md with baseline verification step and enhanced SCORECARD structure for multi-baseline comparisons.
</objective>

<execution_context>
@/Users/evanowen/.claude/get-research-done/workflows/execute-plan.md
@/Users/evanowen/.claude/get-research-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-baseline-orchestration/08-CONTEXT.md
@.planning/phases/08-baseline-orchestration/08-RESEARCH.md
@agents/grd-evaluator.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Step 1.5 Baseline Validation to grd-evaluator.md</name>
  <files>agents/grd-evaluator.md</files>
  <action>
Insert a new step "## Step 1.5: Validate Baseline Availability" between "## Step 1: Load Context" and "## Step 2: Verify Critic Approval".

The new step must include:

1. **Re-validate baselines at evaluation time (safety check):**
   - This is a secondary check - Researcher should have validated at start
   - But baseline could have been deleted between experiment run and evaluation
   - Parse baselines from OBJECTIVE.md (same logic as Researcher)
   - Check if baseline run directories still exist

2. **Handle baseline availability scenarios:**

   **Scenario A: Primary baseline exists**
   - Load metrics.json from baseline run
   - Store for comparison in Step 4
   - Log: "Primary baseline validated: {baseline_name} ({run_path})"

   **Scenario B: Primary baseline missing (was present at experiment start)**
   - WARN: "Primary baseline no longer available - comparison limited"
   - Set `baseline_comparison` to degraded mode
   - Include warning in SCORECARD metadata
   - Do NOT block - proceed with evaluation (baseline was valid when experiment ran)

   **Scenario C: No baselines defined**
   - Log: "No baselines defined in OBJECTIVE.md"
   - Set `baseline_comparison` to null in SCORECARD
   - Proceed without baseline comparison

   **Scenario D: --skip-baseline was used**
   - Check if run metadata indicates `baseline_validation_skipped: true`
   - Log: "Baseline validation was skipped - no comparison available"
   - Include in SCORECARD metadata

3. **Load secondary baseline metrics:**
   - For each secondary baseline defined in OBJECTIVE.md
   - Try to load metrics.json
   - If available, add to comparison list
   - If missing, add to warnings list

4. **Store baseline data for Step 4:**
```python
baseline_data = {
    'primary': {
        'name': baseline_name,
        'run_path': run_path,
        'metrics': loaded_metrics,
        'available': True
    },
    'secondary': [
        {'name': name, 'run_path': path, 'metrics': metrics, 'available': True},
        {'name': missing_name, 'available': False}
    ],
    'warnings': [],
    'validation_skipped': False
}
```
  </action>
  <verify>
Read agents/grd-evaluator.md and verify Step 1.5 exists between Step 1 and Step 2 with all scenarios documented.
  </verify>
  <done>
grd-evaluator.md has Step 1.5 that re-validates baselines at evaluation time and handles all availability scenarios gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance Step 4 for Multi-Baseline Comparison</name>
  <files>agents/grd-evaluator.md</files>
  <action>
Update "## Step 4: Compute Metrics" section to enhance baseline comparison for multi-baseline scenarios.

Find the existing baseline comparison code (around "**Baseline comparison (if baseline_defined: true):**") and replace/enhance with:

```python
# Multi-baseline comparison
if baseline_data['primary']['available'] or any(b['available'] for b in baseline_data['secondary']):
    baseline_comparisons = []

    # Compare against primary baseline
    if baseline_data['primary']['available']:
        primary = baseline_data['primary']
        primary_score = primary['metrics'].get('composite_score', calculate_composite(primary['metrics']))
        improvement = composite_score - primary_score
        improvement_pct = (improvement / primary_score) * 100 if primary_score != 0 else 0

        baseline_comparisons.append({
            'name': primary['name'],
            'type': 'primary',
            'source': 'own_implementation',  # or from OBJECTIVE.md
            'score': primary_score,
            'experiment_score': composite_score,
            'improvement': improvement,
            'improvement_pct': f"{improvement_pct:.1f}%",
            'significant': test_significance(fold_results, primary.get('per_fold', [])),
            'run_path': primary['run_path']
        })

    # Compare against each secondary baseline
    for secondary in baseline_data['secondary']:
        if secondary['available']:
            sec_score = secondary['metrics'].get('composite_score', calculate_composite(secondary['metrics']))
            improvement = composite_score - sec_score
            improvement_pct = (improvement / sec_score) * 100 if sec_score != 0 else 0

            baseline_comparisons.append({
                'name': secondary['name'],
                'type': 'secondary',
                'source': secondary.get('source', 'own_implementation'),
                'score': sec_score,
                'experiment_score': composite_score,
                'improvement': improvement,
                'improvement_pct': f"{improvement_pct:.1f}%",
                'significant': test_significance(fold_results, secondary.get('per_fold', [])),
                'run_path': secondary['run_path']
            })
        else:
            # Log unavailable secondary baseline
            baseline_comparisons.append({
                'name': secondary['name'],
                'type': 'secondary',
                'available': False,
                'note': 'Secondary baseline not available for comparison'
            })

    # Format as comparison table for SCORECARD
    baseline_comparison = {
        'experiment_score': composite_score,
        'baselines': baseline_comparisons,
        'primary_baseline': baseline_data['primary']['name'] if baseline_data['primary']['available'] else None,
        'secondary_baselines': [b['name'] for b in baseline_data['secondary'] if b.get('available', False)],
        'warnings': baseline_data.get('warnings', [])
    }
else:
    baseline_comparison = {
        'experiment_score': composite_score,
        'baselines': [],
        'warnings': ['No baseline comparison available']
    }
```

Also add a helper function for composite score calculation if baseline metrics don't include it:
```python
def calculate_composite(metrics: dict) -> float:
    """Calculate composite score from individual metrics if not pre-computed."""
    # Use weights from OBJECTIVE.md if available
    # Fall back to equal weights if not
    if 'composite_score' in metrics:
        return metrics['composite_score']

    metric_values = [v for k, v in metrics.items()
                     if k not in ['timestamp', 'run_id', 'iteration']]
    return sum(metric_values) / len(metric_values) if metric_values else 0.0
```
  </action>
  <verify>
Read agents/grd-evaluator.md Step 4 section and verify multi-baseline comparison logic is present.
  </verify>
  <done>
grd-evaluator.md Step 4 computes comparison against all available baselines with improvement metrics and significance testing.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update SCORECARD.json Structure in Step 5</name>
  <files>agents/grd-evaluator.md</files>
  <action>
Update "## Step 5: Generate SCORECARD.json" to reflect the new multi-baseline comparison structure.

Replace the existing `baseline_comparison` section in the example SCORECARD.json with:

```json
"baseline_comparison": {
  "experiment_score": 0.857,
  "baselines": [
    {
      "name": "random_classifier",
      "type": "primary",
      "source": "own_implementation",
      "score": 0.501,
      "experiment_score": 0.857,
      "improvement": 0.356,
      "improvement_pct": "71.1%",
      "significant": true,
      "run_path": "experiments/run_001_random_classifier/"
    },
    {
      "name": "prior_best_model",
      "type": "secondary",
      "source": "own_implementation",
      "score": 0.823,
      "experiment_score": 0.857,
      "improvement": 0.034,
      "improvement_pct": "4.1%",
      "significant": false,
      "run_path": "experiments/run_002_prior_best/"
    },
    {
      "name": "literature_benchmark",
      "type": "secondary",
      "source": "literature_citation",
      "score": 0.840,
      "improvement": 0.017,
      "improvement_pct": "2.0%",
      "significant": "not_tested",
      "note": "Literature baseline from cited paper (no per-fold data for significance test)"
    }
  ],
  "primary_baseline": "random_classifier",
  "secondary_baselines": ["prior_best_model", "literature_benchmark"],
  "warnings": []
},
```

Also add metadata field for validation state:
```json
"baseline_validation": {
  "researcher_validated": true,
  "evaluator_validated": true,
  "validation_skipped": false,
  "data_hash_match": true,
  "notes": []
}
```

Update the return format section to reflect multi-baseline comparison in output:
```markdown
**Baseline Comparison:**
| Baseline | Type | Score | Improvement | Significant |
|----------|------|-------|-------------|-------------|
| {name} | {primary|secondary} | {score} | +{improvement} ({pct}%) | {yes|no|not_tested} |
```
  </action>
  <verify>
Read agents/grd-evaluator.md Step 5 and return format section to verify multi-baseline SCORECARD structure.
  </verify>
  <done>
grd-evaluator.md generates SCORECARD.json with multi-baseline comparison table showing experiment vs each available baseline.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Verify Step 1.5 exists:
```bash
grep -A 5 "Step 1.5" agents/grd-evaluator.md
```

2. Verify multi-baseline comparison in Step 4:
```bash
grep -A 20 "Multi-baseline comparison" agents/grd-evaluator.md
```

3. Verify SCORECARD structure:
```bash
grep -A 30 '"baseline_comparison"' agents/grd-evaluator.md
```
</verification>

<success_criteria>
- [ ] Step 1.5 validates baseline availability before SCORECARD generation
- [ ] Step 4 computes comparison against all available baselines
- [ ] SCORECARD structure supports multi-baseline comparison table
- [ ] Missing baselines handled gracefully with warnings
- [ ] Return format shows baseline comparison table
</success_criteria>

<output>
After completion, create `.planning/phases/08-baseline-orchestration/08-02-SUMMARY.md`
</output>
