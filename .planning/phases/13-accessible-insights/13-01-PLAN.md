---
phase: 13-accessible-insights
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .claude/get-research-done/lib/templates/insights_summary.md.j2
  - .claude/get-research-done/lib/templates/data_report.md.j2
  - .claude/get-research-done/lib/insights.py
autonomous: true

must_haves:
  truths:
    - "insights.py can generate plain English explanations for every statistic"
    - "INSIGHTS_SUMMARY.md template produces scannable output with TL;DR at top"
    - "Severity-aware language adapts to critical vs moderate vs healthy findings"
    - "Recommendations include specific next steps with example code"
    - "LLM prompts are generated based on actual dataset findings"
  artifacts:
    - path: ".claude/get-research-done/lib/templates/insights_summary.md.j2"
      provides: "Plain English insights template with inverted pyramid structure"
      contains: "TL;DR"
    - path: ".claude/get-research-done/lib/templates/data_report.md.j2"
      provides: "Technical DATA_REPORT.md template"
      contains: "Generated by GRD Insights"
    - path: ".claude/get-research-done/lib/insights.py"
      provides: "Insight generation orchestrator"
      exports: ["generate_insights"]
      min_lines: 200
  key_links:
    - from: ".claude/get-research-done/lib/insights.py"
      to: "templates/*.j2"
      via: "Jinja2 Environment.get_template()"
      pattern: "env\\.get_template"
    - from: ".claude/get-research-done/lib/insights.py"
      to: ".claude/get-research-done/lib/quick.py"
      via: "Reusing analysis functions"
      pattern: "from.*quick import|from quick import"
---

<objective>
Create the core insights generation module with Jinja2 templates for plain English data insights.

Purpose: Enable business analysts to understand data without technical jargon, with "What This Means" explanations for every statistic and actionable recommendations.

Output:
- `insights.py` module with `generate_insights()` function
- `insights_summary.md.j2` template for plain English summary (displayed to user)
- `data_report.md.j2` template for technical report (saved to file)
</objective>

<execution_context>
@/Users/evanowen/.claude/get-shit-done/workflows/execute-plan.md
@/Users/evanowen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-accessible-insights/13-CONTEXT.md
@.planning/phases/13-accessible-insights/13-RESEARCH.md
@.claude/get-research-done/lib/quick.py
@.claude/get-research-done/lib/formatters.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Jinja2 templates for insights generation</name>
  <files>
    .claude/get-research-done/lib/templates/insights_summary.md.j2
    .claude/get-research-done/lib/templates/data_report.md.j2
  </files>
  <action>
Create the templates directory and two Jinja2 templates:

**1. insights_summary.md.j2 (Plain English summary - displayed to user)**

Structure follows inverted pyramid from RESEARCH.md:
```
# Data Insights Summary

**Generated:** {{ timestamp }}
**Dataset:** {{ analysis.n_rows|format_number }} rows x {{ analysis.n_cols }} columns

---

## TL;DR

[Critical finding with action OR "Data looks healthy"]

---

## 5 Things to Know

**1. [Headline that tells the story]**
[Explanation] [Action if needed]
...

---

## Data Overview

| Metric | Value | What This Means |
|--------|-------|-----------------|
| Rows | X | [Explanation] |
...

---

## Critical Issues (if any)

### [Column] - [Issue type emoji]
**Problem:** [Description]
**What this means:** [Plain English impact]
**What to do:** [Specific options with code examples]

---

## Recommendations

### [Priority emoji] [Action title]
**Why:** [Reason]
**How to address:** [Code examples]

---

## Dig Deeper (LLM Prompts)

Copy and paste these into ChatGPT, Claude, or any LLM:

**1. [Prompt title]**
```
[Prompt text with specific dataset context]
```

---

*Full technical report: DATA_REPORT.md*
*Generated by GRD Accessible Insights*
```

Use Jinja2 conditionals for severity-aware language per RESEARCH.md Pattern 1:
- `{% if missing_pct > 50 %}` -> Critical emoji and strong language
- `{% elif missing_pct > 20 %}` -> Warning emoji and moderate language
- `{% else %}` -> Healthy emoji and positive framing

Add custom filters: `|format_number` for comma-separated numbers, `|format_pct` for percentages

**2. data_report.md.j2 (Technical report - saved to file)**

Same structure as quick.py's generate_markdown_report() but:
- Remove "Quick Explore Mode" header
- Add "Generated by GRD Insights" branding
- Include full technical statistics
- Keep same column summary table format for consistency
  </action>
  <verify>
```bash
# Verify templates exist and have correct structure
ls -la .claude/get-research-done/lib/templates/*.j2
grep -l "TL;DR" .claude/get-research-done/lib/templates/insights_summary.md.j2
grep -l "What This Means" .claude/get-research-done/lib/templates/insights_summary.md.j2
grep -l "Dig Deeper" .claude/get-research-done/lib/templates/insights_summary.md.j2
```
  </verify>
  <done>
- insights_summary.md.j2 exists with TL;DR, "5 Things to Know", "What This Means" explanations, severity-aware conditionals, recommendations section, and LLM prompts section
- data_report.md.j2 exists with technical report structure
- Both templates use consistent formatting and GRD branding
  </done>
</task>

<task type="auto">
  <name>Task 2: Create insights.py module with narrative generation</name>
  <files>
    .claude/get-research-done/lib/insights.py
  </files>
  <action>
Create insights.py with the following structure based on RESEARCH.md code examples:

**Main function:**
```python
def generate_insights(
    df: pd.DataFrame,
    target_col: Optional[str] = None,
    output_dir: str = "."
) -> None:
    """
    Generate accessible insights for business analyst audience.

    Creates two outputs:
    1. DATA_REPORT.md - Technical report (saved to file)
    2. INSIGHTS_SUMMARY.md - Plain English summary (displayed + saved)
    """
```

**Analysis functions (reuse from quick.py where possible):**
- `identify_critical_issues(df)` - >50% missing, constant columns, leakage
- `identify_moderate_issues(df)` - 20-50% missing, outliers
- `identify_positive_findings(df)` - Things that look healthy
- `generate_key_takeaways(df, issues)` - "5 Things to Know" headlines
- `generate_recommendations(df, target_col)` - Specific, actionable with code examples
- `generate_llm_prompts(df, target_col)` - 3-5 contextual prompts

**Explanation helpers (from RESEARCH.md Example 6):**
- `explain_row_count(n_rows)` - Plain English interpretation
- `explain_column_count(n_cols)` - Plain English interpretation
- `explain_missing(missing_pct)` - Plain English interpretation
- `explain_memory(memory_mb)` - Plain English interpretation

**Statistical translations (inline pattern from RESEARCH.md):**
```python
STAT_TRANSLATIONS = {
    'standard_deviation': 'Standard deviation (how spread out values are)',
    'skewness': 'Skewness (symmetry of distribution)',
    'correlation': 'Correlation (how strongly two things move together)',
    'cardinality': 'Cardinality (number of unique values)',
    'outliers': 'Outliers (extreme values far from typical)'
}
```

**Jinja2 setup:**
- Create Environment with FileSystemLoader pointing to templates/
- Add custom filters: `format_number`, `format_pct`
- Use `trim_blocks=True, lstrip_blocks=True` for clean output

**Key implementation details:**
- Import from quick.py: reuse analysis logic where applicable
- Severity thresholds: >50% = critical, 20-50% = moderate, <20% = healthy
- Recommendations: Each includes specific code examples per CONTEXT decisions
- LLM prompts: Max 5, based on actual findings (missing data, outliers, target)
  </action>
  <verify>
```bash
# Verify module structure
python3 -c "
import sys
sys.path.insert(0, '.claude/get-research-done/lib')
from insights import generate_insights, identify_critical_issues, generate_recommendations, generate_llm_prompts
print('imports: OK')
"

# Verify key functions exist
grep -c "def generate_insights" .claude/get-research-done/lib/insights.py
grep -c "def identify_critical_issues" .claude/get-research-done/lib/insights.py
grep -c "def generate_recommendations" .claude/get-research-done/lib/insights.py
grep -c "def generate_llm_prompts" .claude/get-research-done/lib/insights.py
grep -c "STAT_TRANSLATIONS" .claude/get-research-done/lib/insights.py
```
  </verify>
  <done>
- insights.py module created with generate_insights() as main entry point
- Analysis functions identify critical, moderate, and positive findings
- Explanation helpers translate statistics to plain English
- Recommendations include specific code examples
- LLM prompts generated based on dataset context
- Jinja2 environment configured with custom filters
  </done>
</task>

<task type="auto">
  <name>Task 3: Test insights generation with sample data</name>
  <files>
    (no files created - verification only)
  </files>
  <action>
Create a quick test to verify the insights generation works end-to-end:

```python
import pandas as pd
import numpy as np
import sys
sys.path.insert(0, '.claude/get-research-done/lib')
from insights import generate_insights

# Create test DataFrame with known issues
np.random.seed(42)
df = pd.DataFrame({
    'id': range(1000),
    'age': np.random.normal(35, 10, 1000),
    'income': np.random.exponential(50000, 1000),
    'category': np.random.choice(['A', 'B', 'C'], 1000),
    'missing_col': [np.nan if i < 300 else i for i in range(1000)],
    'constant_col': ['same'] * 1000
})

# Add some outliers
df.loc[0:10, 'income'] = 1000000

# Generate insights
generate_insights(df, target_col=None, output_dir='/tmp')

# Verify outputs
import os
assert os.path.exists('/tmp/DATA_REPORT.md'), "DATA_REPORT.md not created"
assert os.path.exists('/tmp/INSIGHTS_SUMMARY.md'), "INSIGHTS_SUMMARY.md not created"

# Check content
with open('/tmp/INSIGHTS_SUMMARY.md') as f:
    content = f.read()
    assert 'TL;DR' in content, "Missing TL;DR section"
    assert 'What This Means' in content or 'What this means' in content, "Missing explanations"
    assert 'missing_col' in content or 'constant_col' in content, "Critical issues not detected"

print("All tests passed!")
```

Run this test to verify:
1. Both output files are created
2. TL;DR section present
3. "What This Means" explanations present
4. Critical issues (high missing, constant column) detected
  </action>
  <verify>
```bash
# Run the test script
python3 << 'EOF'
import pandas as pd
import numpy as np
import sys
import os

sys.path.insert(0, '.claude/get-research-done/lib')
from insights import generate_insights

# Create test DataFrame
np.random.seed(42)
df = pd.DataFrame({
    'id': range(1000),
    'age': np.random.normal(35, 10, 1000),
    'income': np.random.exponential(50000, 1000),
    'category': np.random.choice(['A', 'B', 'C'], 1000),
    'missing_col': [np.nan if i < 300 else i for i in range(1000)],
    'constant_col': ['same'] * 1000
})
df.loc[0:10, 'income'] = 1000000

# Generate insights
generate_insights(df, target_col=None, output_dir='/tmp')

# Verify outputs exist
assert os.path.exists('/tmp/DATA_REPORT.md'), "DATA_REPORT.md not created"
assert os.path.exists('/tmp/INSIGHTS_SUMMARY.md'), "INSIGHTS_SUMMARY.md not created"

print("Test passed: Both output files created")
EOF
```
  </verify>
  <done>
- Test script runs without errors
- DATA_REPORT.md created in output directory
- INSIGHTS_SUMMARY.md created in output directory
- Content includes TL;DR and explanations
  </done>
</task>

</tasks>

<verification>

**Template verification:**
```bash
ls -la .claude/get-research-done/lib/templates/
cat .claude/get-research-done/lib/templates/insights_summary.md.j2 | head -50
```

**Module verification:**
```bash
python3 -c "
import sys
sys.path.insert(0, '.claude/get-research-done/lib')
from insights import generate_insights
print('Module imports successfully')
"
```

**Integration test:**
```bash
# Full end-to-end test with sample data
python3 .claude/get-research-done/lib/insights.py --test 2>/dev/null || echo "No --test flag (OK)"
```

</verification>

<success_criteria>

- [ ] templates/ directory exists under lib/
- [ ] insights_summary.md.j2 has TL;DR, "5 Things to Know", explanations, recommendations, LLM prompts
- [ ] data_report.md.j2 has technical report structure
- [ ] insights.py has generate_insights() function
- [ ] insights.py has analysis functions (critical, moderate, positive)
- [ ] insights.py has explanation helpers for plain English
- [ ] insights.py has recommendation generation with code examples
- [ ] insights.py has LLM prompt generation
- [ ] Test script passes: both output files created

</success_criteria>

<output>
After completion, create `.planning/phases/13-accessible-insights/13-01-SUMMARY.md`
</output>
